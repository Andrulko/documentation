# Process ZMQ events in near real-time with Apache Flink

**When you subscribe to ZMQ events, you receive near real-time Tangle data from a node. To process this data, you can use a stream processing framework such as the open-source [Apache Flink](https://flink.apache.org/).**

This guide uses the [Flink Tangle source library](https://github.com/Citrullin/flink-tangle-source) to process ZMQ data with Flink. 

This library uses the [ZeroMQMessageParser](https://github.com/Citrullin/tangle-streaming/blob/master/src/main/scala/org/iota/tangle/stream/ZeroMQMessageParser.scala) from the [Tangle streaming library](https://github.com/Citrullin/tangle-streaming) to parse the raw event messages into class instances.

All ZMQ event messages are wrapped in classes that are generated by [protobuf schema files](https://github.com/Citrullin/tangle-streaming/tree/master/src/main/protobuf). All protobuf messages and attributes are also available in Flink.

Because this library uses the ZMQ API, all [ZMQ events](../references/zmq-events.md) are available for processing.

:::info:
The Tangle streaming libraries in this guide are not recommended for production environments.

Feel free to contribute to the libraries, so that they eventually become production ready.
:::

## Prerequisites

To complete this guide, you need the following:

- **Operating system:** Linux, MacOS, BSD or Windows
- **RAM:** 2GB
- **Storage:** 10GB free space

## Download and install the libraries

This guide uses the Scala programming language with the sbt build tool.

If you want to use Scala in a Java Runtime Environment (JRE), you need to add the Scala library to [Maven](https://mvnrepository.com/artifact/org.scala-lang/scala-library) or [sbt](http://xerial.org/blog/2014/03/24/sbt/).
 
This [Artima guide](https://www.artima.com/pins1ed/combining-scala-and-java.html) describes how you can use Scala in a JRE.

1. [Install Java](http://openjdk.java.net/install/). Because Scala uses the Java virtual machine, you must install Java 8 or higher.

2. [Install sbt](https://www.scala-sbt.org/1.x/docs/Setup.html)

3. Clone the libraries

  ```bash
  git clone https://github.com/Citrullin/tangle-streaming.git
  git clone https://github.com/Citrullin/flink-tangle-source
  ```

4. Change into the `tangle-streaming` directory and initialize the REPL (Read-Evaluate-Print Loop)

  ```bash
  cd tangle-streaming && sbt
  ```

5. In the REPL, build the library

  ```bash
  compile
  publishLocal
  ```

6. Press **Ctrl** + **C** to terminate the REPL

7. Change into the `flink-tangle-source` directory and initialize the REPL

  ```bash
  cd ../flink-tangle-source && sbt
  ```

8. In the REPL, build the library

  ```bash
  compile
  publishLocal
  ```

9. Add the dependencies to the `build.sbt` file

  ```scala
  libraryDependencies += "org.iota" %% "flink-tangle-source" % "0.0.1",
  ```

:::success:
Now that you've downloaded and installed the libraries you can start using them to process ZMQ data.
We have [some examples available here](https://github.com/iota-community/flink-tangle-examples).
:::

## Process the top 10 most used addresses in the last hour

You can use the data in ZMQ event streams to find out the top 10 most used addresses in the last hour.

This code in this guide is available in the `MostUsedAddresses.scala` file on [this IOTA community GitHub repository](https://github.com/iota-community/flink-tangle-examples).

## Prerequisites

To complete this guide, you need the following:

* [Downloaded and installed libraries](#download-and-install-the-libraries)
* Access to a node that enables [ZMQ](../references/iri-configuration-options.md)

:::info:
[Tanglebeat provides a list of public nodes that have ZMQ enabled.](http://tanglebeat.com/page/internals).

At the moment, cIRI does not support the ZMQ API.
:::

If you are not familiar with Flink, read [this documentation](https://ci.apache.org/projects/flink/flink-docs-master/tutorials/datastream_api.html#writing-a-flink-program).

---

1. Set up the stream by connecting to a node

  ```scala
  val unconfirmedMessageDescriptorName = UnconfirmedTransactionMessage.scalaDescriptor.fullName
  val zeroMQHost = "HOSTNAME|IP"
  val zeroMQPort = config.getInt(ConfigurationKeys.ZeroMQ.port)
  val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment

  val stream = env.addSource(new TangleSource(zeroMQHost, zeroMQPort, ""))
  ```
  :::info:
  Here, we connect to a node by its hostname and port. We could subscribe to a specific topic such as the [tx](../references/zmq-events.md#tx) event:

  ```scala
  val stream = env.addSource(new TangleSource(zeroMQHost, zeroMQPort, "tx"))
  ```
  :::

2. Filter the `GeneratedMessage` messages with the [protobuf descriptor](https://developers.google.com/protocol-buffers/docs/reference/cpp/google.protobuf.descriptor)

  ```scala
  val filteredStream = stream
    .filter(_.companion.scalaDescriptor.fullName == unconfirmedMessageDescriptorName)
  ```

3. Make sure that the stream contains only `UnconfirmedTransactionMessage` events

  ```scala
  val unconfirmedTransactionStream = filteredStream.map(_ match {
          case m: UnconfirmedTransactionMessage => Some(m)
          case _ => None
        })
        .map(_.get)
  ```

  :::warning:Warning
  This library is only a proof of concept.

  You should never use `_.get` because you can run into `NullPointer` exceptions. Instead, use [getOrElse](https://www.tutorialspoint.com/scala/scala_options.htm).

  You could also implement a filter in the library so that the correct type is returned. Such a filter would make the type checking obsolete.
  :::

4. Filter out the addresses and keep a count of them. For this example, we count every address in each transaction

  ```scala
  val addressOnlyStream = unconfirmedTransactionStream.map(e => (e.address, 1L))
  ```

  :::info:
  To detect spent addresses, you could filter on outputs. 
  If you want to do that, you have to apply a filter with value > 0 or value < 0.
  :::

5. To count the addresses, key the stream by the address 

  ```scala
  val keyedStream = addressOnlyStream.keyBy(_._1)
  ```

  :::info:
  For more complex use cases you can use [windowAll](https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/operators/windows.html#window-assigners).
  :::
 
  Now, we have a keyed stream where every partition contains tuples of the same kind.

6. Calculate the number of transactions for each address within one hour

  ```scala
  val keyedTimedWindow = keyedStream.timeWindow(Time.minutes(60), Time.seconds(30))
  ```

  :::info:
  Here, we use an overlapping [sliding window](https://ci.apache.org/projects/flink/flink-docs-master/dev/stream/operators/windows.html#sliding-windows) with an window slide of 30 seconds.
  :::

7. Aggregate each partition

  ```scala
  val aggregatedKeyedTimeWindow = timedWindow.reduce((a, b) => (a._1, a._2 + b._2))
  ```

  :::info:
  If you have more complex operations, you can use aggregation functions.
  You can find a more complex example in the [BundleAggregation.scala](https://github.com/iota-community/flink-tangle-examples/blob/master/src/main/scala/org/iota/tangle/flink/examples/BundleAggregation.scala) file. This example combines incoming transaction into a bundle and splits them into unconfirmed bundles and unconfirmed bundles that have been reattached.
  :::

8. Find the top 10 addresses and aggregate them

  ```scala
  val timeWindowAll = aggregatedKeyedTimeWindow
        .timeWindowAll(Time.seconds(1))
  ```

  :::info: 
  Because we used an overlapping sliding window on our partitions, the time here is not important. So, we just use one second.
  :::

  Now, each partition has one tuple in the structure (ADDRESS, AMOUNT_OF_TRANSACTIONS).

9. Use an aggregation function to find out which addresses are used the most

  ```scala
  val mostUsedStream = timeWindowAll.aggregate(new MostUsedAddressesAggregator(10))
  ```

  :::info:
  This function returns a list from the most used address to the least used address.

  We pass the number 10 to the function so that it returns only the top 10 most used addresses.
  :::

  **The MostUsedAddressesAggregator class**

  ```scala
  class MostUsedAddressesAggregator(number: Int) extends AggregateFunction[(String, Long), Map[String, Long], List[(String, Long)]]
  {
    override def add(value: (String, Long), accumulator: Map[String, Long]): Map[String, Long] = {
      accumulator ++ Map(value._1 -> (value._2 + accumulator.getOrElse(value._1, 0L)))
    }

    override def createAccumulator(): Map[String, Long] = Map()

    override def getResult(accumulator: Map[String, Long]): List[(String, Long)] =
      accumulator.toList.sortWith(_._2 > _._2).take(number)

    override def merge(a: Map[String, Long], b: Map[String, Long]): Map[String, Long] = {
      val seq = a.toSeq ++ b.toSeq
      val grouped = seq.groupBy(_._1)
      val mapWithCounts = grouped.map{case (key, value) => (key, value.map(_._2))}

      mapWithCounts.map{case (key, value) => (key, value.sum)}
    }
  }
  ```

10. Print the list and execute the program

  ```scala
  mostUsedStream.print()

      // execute program
      env.execute("Most used addresses")
  ```

:::success: Congratulations :tada:
You're now processing near-real-time data from the Tangle.
:::

